% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/send_to_dai.R
\name{dai_async_tab}
\alias{dai_async_tab}
\title{OCR documents asynchronously and extract table data}
\usage{
dai_async_tab(
  files,
  filetype = "pdf",
  dest_folder = NULL,
  bucket = Sys.getenv("GCS_DEFAULT_BUCKET"),
  proj_id = get_project_id(),
  loc = "eu",
  token = dai_token(),
  pps = 100
)
}
\arguments{
\item{files}{A vector or list of pdf filepaths in a Google Cloud Storage bucket. Filepaths must include all parent bucket folder(s) except the bucket name.}

\item{filetype}{Either "pdf", "gif", or "tiff". If \code{files} is a vector, all elements must be of the same type.}

\item{dest_folder}{The name of the bucket subfolder where you want the json output.}

\item{bucket}{The name of your bucket. Not necessary if you have set a default bucket with googleCloudStorageR::gcs_global_bucket().}

\item{proj_id}{Your Google Cloud Services project id.}

\item{loc}{A two-letter region code ("eu" or "us"). Defaults to "eu".}

\item{token}{An authentication token generated by \code{dai_auth()} or another auth function.}

\item{pps}{An integer from 1 to 100 for the desired number of pages per shard in the JSON output. Defaults to 100.}
}
\value{
A list of HTTP responses.
}
\description{
This function sends files from your Google Storage bucket to Google Document AI v1beta2 for asynchronous (offline) processing. The output is delivered to your bucket as json files. This function accesses a different endpoint than the main \code{dai_async()} function, one that has less language support, but extracts table data (which \code{dai_async()} currently does not). This function will be deprecated if/when the v1 endpoint incorporates table extraction.
}
\details{
Requires a Google Cloud access token (\code{google_token}) and a certain amount of configuration in RStudio; see vignettes for details. For long pdf documents, the json output is divided into separate files (shards) of 20 pages each. Maximum pdf document length is 2000 pages. The maximum number of pages in active processing is 10,000. The function waits 10 seconds between each document submission, so I recommend using RStudio's "Jobs" functionality if you are processing a lot of files.

NOTE: The function in its current form is a placeholder for a future function that allows for real batch processing (as opposed to an iterated single submission).
}
\examples{
\dontrun{
# with daiR configured on your system, several parameters are automatically provided,
# and you can pass simple calls, such as:
dai_async("my_document.pdf")

# NB: Include all parent bucket folders (but not the bucket name) in the filepath:
dai_async("for_processing/pdfs/my_document.pdf")

# Bulk process by passing a vector of filepaths in the files argument:
dai_async(my_files)

# Specify a bucket subfolder for the json output:
dai_async(my_files, dest_folder = "processed")
}
}
